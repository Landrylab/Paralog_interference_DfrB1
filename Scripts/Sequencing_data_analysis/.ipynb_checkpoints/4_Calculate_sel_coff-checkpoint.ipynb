{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 004 Sequencing data processing and calculations for selection coeffitient: \n",
    "\n",
    "This script calculates the selection coefficient for each codon in each library according to the following equation (Cisneros et al. 2023):\n",
    "\n",
    "    selection_coefficient= (log2((Nmut[t10]/median_Nwt[t10])/(Nmut[t0]/median_Nwt[t0])))/k\n",
    "    \n",
    "    N=number of reads (abundance that stand for the counts of the mutant by the total number of counts for all the variants in the sample)\n",
    "    k=number of generations after t (=10)\n",
    "\n",
    "This script assumes the following folder layout\n",
    "- Home folder\n",
    "    - Scripts folder with this script\n",
    "    - Data/Complete_codons dataframes that containing the dataframes with the codon counts and abundances for each sample in each run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import modules and check versions\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.__name__, pd.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__name__, np.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "print(matplotlib.__name__, matplotlib.__version__)\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "print(scipy.__name__, scipy.__version__)\n",
    "\n",
    "import re\n",
    "print(re.__name__, re.__version__)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGE THE PATH TO YOUR LOCAL FOLDER TO MAKE THE WHOLE NOTEBOOK WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/path/Novaseq_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define some helper functions that will help us for the further analysis\n",
    "def reverse_complement(dna):\n",
    "    \"\"\" function that reverse complements DNA\n",
    "    dna: input dna sequence\n",
    "    \"\"\"\n",
    "    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "    return ''.join([complement[base] for base in dna[::-1]])\n",
    "\n",
    "def get_dict_of_seq(fasta_file):\n",
    "    \"\"\" function that converts a fasta file to a dictionnary of sequences\n",
    "    fasta_file: the input fasta file\n",
    "    \"\"\"\n",
    "    \n",
    "    file_fasta_dict = {}\n",
    "    # output dict of imported seqs\n",
    "    \n",
    "    with open(fasta_file, 'r') as fasta:    \n",
    "        for line in fasta:\n",
    "            # loops through the file\n",
    "\n",
    "            if line.startswith('>') == True:\n",
    "                seq_info = line.strip('>').strip('\\n').split('\\t')[0]\n",
    "                file_fasta_dict[seq_info] = ''\n",
    "                # checks if seq header\n",
    "\n",
    "            else:\n",
    "                file_fasta_dict[seq_info] += line.strip('\\n')\n",
    "                # If not, append nt to current seq\n",
    "                \n",
    "    return file_fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a directoty to save the selection coeffitient files\n",
    "\n",
    "sel_coff_prefix = './Analysis/Sel_coff/'\n",
    "temp_files_prefix= \"./intermediate_temporary_dataframes/\"\n",
    "\n",
    "os.makedirs(sel_coff_prefix, exist_ok = True)\n",
    "os.makedirs(temp_files_prefix, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the genetic code\n",
    "codons = [\n",
    "  'ATA', 'ATC', 'ATT', 'ATG',\n",
    "  'ACA', 'ACC', 'ACG', 'ACT',\n",
    "  'AAC', 'AAT', 'AAA', 'AAG',\n",
    "  'AGC', 'AGT', 'AGA', 'AGG',\n",
    "  'CTA', 'CTC', 'CTG', 'CTT',\n",
    "  'CCA', 'CCC', 'CCG', 'CCT',\n",
    "  'CAC', 'CAT', 'CAA', 'CAG',\n",
    "  'CGA', 'CGC', 'CGG', 'CGT',\n",
    "  'GTA', 'GTC', 'GTG', 'GTT',\n",
    "  'GCA', 'GCC', 'GCG', 'GCT',\n",
    "  'GAC', 'GAT', 'GAA', 'GAG',\n",
    "  'GGA', 'GGC', 'GGG', 'GGT',\n",
    "  'TCA', 'TCC', 'TCG', 'TCT',\n",
    "  'TTC', 'TTT', 'TTA', 'TTG',\n",
    "  'TAC', 'TAT', 'TAA', 'TAG',\n",
    "  'TGC', 'TGT', 'TGA', 'TGG'\n",
    "]\n",
    "\n",
    "residues = [\n",
    "  'I', 'I', 'I', 'M',\n",
    "  'T', 'T', 'T', 'T',\n",
    "  'N', 'N', 'K', 'K',\n",
    "  'S', 'S', 'R', 'R',\n",
    "  'L', 'L', 'L', 'L',\n",
    "  'P', 'P', 'P', 'P',\n",
    "  'H', 'H', 'Q', 'Q',\n",
    "  'R', 'R', 'R', 'R',\n",
    "  'V', 'V', 'V', 'V',\n",
    "  'A', 'A', 'A', 'A',\n",
    "  'D', 'D', 'E', 'E',\n",
    "  'G', 'G', 'G', 'G',\n",
    "  'S', 'S', 'S', 'S',\n",
    "  'F', 'F', 'L', 'L',\n",
    "  'Y', 'Y', '*', '*',\n",
    "  'C', 'C', '*', 'W'\n",
    "]\n",
    "\n",
    "gen_code = {'Codon': codons, 'Residue': residues}\n",
    "genetic_code = pd.DataFrame(data=gen_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### I will try to merge the codon and residues dataframe\n",
    "\n",
    "### First I need to define the genetic code\n",
    "codontable_standard = {\n",
    "    'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',\n",
    "    'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "    'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',\n",
    "    'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',\n",
    "    'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "    'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "    'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "    'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "    'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',\n",
    "    'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "    'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',\n",
    "    'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "    'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "    'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',\n",
    "    'TAC':'Y', 'TAT':'Y', 'TAA':'*', 'TAG':'*',\n",
    "    'TGC':'C', 'TGT':'C', 'TGA':'*', 'TGG':'W',\n",
    "    }\n",
    "\n",
    "### Then I define a function that converts the codon in a cell for the aa that is encoded\n",
    "\n",
    "# Create a function for translation\n",
    "\n",
    "def translate_codon(codon):\n",
    "    try:\n",
    "        return codontable_standard.get(codon, 'Unknown')\n",
    "    except:\n",
    "        return 'Error'\n",
    "    \n",
    "    \n",
    "### Define also a dictionary for the WT sequence of DfrB1\n",
    "\n",
    "wt_codons = {10: 'AAT',11: 'CCA',12: 'GTT',13: 'GCT',14: 'GGC',15: 'AAT',16: 'TTT',17: 'GTA',18: 'TTC',19: 'CCA',\n",
    "             20: 'TCG',21: 'GAC',22: 'GCC',23: 'ACG',24: 'TTT',25: 'GGT',26: 'ATG',27: 'GGA',28: 'GAT',29: 'CGC',30: 'GTG',31: 'CGC',32: 'AAG',33: 'AAA',34: 'TCC',\n",
    "             35: 'GGC',36: 'GCC',37: 'GCC',38: 'TGG',39: 'CAA',40: 'GGT',41: 'CAG',42: 'ATT',43: 'GTC',44: 'GGG',45: 'TGG',46: 'TAC',47: 'TGC',48: 'ACA',49: 'AAT',\n",
    "             50: 'TTG',51: 'ACC',52: 'CCC',53: 'GAA',54: 'GGC',55: 'TAC',56: 'GCC',57: 'GTC',58: 'GAG',59: 'TCT',60: 'GAG',61: 'GCT',62: 'CAC',63: 'CCA',64: 'GGC',\n",
    "             65: 'TCA',66: 'GTA',67: 'CAG',68: 'ATT',69: 'TAT',70: 'CCT',71: 'GTT',72: 'GCG',73: 'GCG',74: 'CTT',75: 'GAA',76: 'CGC',77: 'ATC',78: 'AAC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the data. Here I can add as many runs I've done for the project \n",
    "Run1=pd.read_csv(\"./Data/Complete_codons_DF_run1.csv\")\n",
    "Run2=pd.read_csv(\"./Data/Complete_codons_DF_run2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Concatenate all the run datafranes into a single one \n",
    "Novaseq_data = pd.concat([Run1, Run2]) ## Add here all the runs that have been done in the project\n",
    "Novaseq_data = Novaseq_data.reset_index(drop=True)\n",
    "\n",
    "# Apply translation to the dataframe column for library encoded codons\n",
    "Novaseq_data['Residue'] = Novaseq_data['Codon'].apply(translate_codon)\n",
    "    \n",
    "# Apply translation to the dataframe column for WT codons\n",
    "Novaseq_data['WT_Residue'] = Novaseq_data['WT_Codon'].apply(translate_codon)\n",
    "\n",
    "print(\"The length of Novaseq Data \" + str(len(Novaseq_data)) + \" should be equal to \" + str(len(Run1)+len(Run2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the data by removing the stop codon at the last position\n",
    "Novaseq_data = Novaseq_data[Novaseq_data['Position'] != 70]\n",
    "\n",
    "# Add +9 to the 'Position' column to start from position 10 where actually my DMS starts\n",
    "Novaseq_data['Position'] = Novaseq_data['Position'] + 9\n",
    "\n",
    "# Rename a specific column\n",
    "Novaseq_data = Novaseq_data.rename(columns={'Count': 'read_counts'})\n",
    "# #Calculate the log2 for the codon counts\n",
    "Novaseq_data[\"log2_counts\"]=np.log2(Novaseq_data[\"read_counts\"]+1)\n",
    "\n",
    "# Test if the calculated abundances are correct. The sum of abundances per sample should be equal to 1\n",
    "\n",
    "### I will test it for t0 because it is easier and all is done in the same way\n",
    "Sum=Novaseq_data[Novaseq_data['Timepoint'] == 0]\n",
    "\n",
    "### Remove also lane 2\n",
    "Sum=Sum[Sum['Lane'] == 1]\n",
    "\n",
    "Sum=Sum[\"read_abundance\"].sum()\n",
    "\n",
    "### But the sum has to be divided by all the samples in my dataframe, that in this case is 18 (13 for Run1 and 5 for Run2)\n",
    "\n",
    "Sum=Sum/18\n",
    "\n",
    "print(\"The mean abundance per sample \" + str(Sum) + \" should be equal to 1. IF NOT, eliminate the read abundance and recalculate it later on.\")\n",
    "\n",
    "# Remove read abunance column \n",
    "Novaseq_data=Novaseq_data.drop(columns= ['read_abundance'])\n",
    "\n",
    "# Save the file \n",
    "Novaseq_data.to_csv(\"./All_run_masterfile.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Novaseq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### I want to do a wide format dataframe separating all the variable columns bu timepoint 0 and 10\n",
    "\n",
    "# Drop the 'ID' column\n",
    "Novaseq_data_1 = Novaseq_data.drop('ID', axis=1)\n",
    "\n",
    "## Split the data into t0 and t10\n",
    "Novaseq_data_t0 = Novaseq_data_1[Novaseq_data_1['Timepoint'] == 0]\n",
    "Novaseq_data_t10 = Novaseq_data_1[Novaseq_data_1['Timepoint'] == 10]\n",
    "\n",
    "# Drop the 'Timepoint' column for each dataframe\n",
    "Novaseq_data_t0_1 = Novaseq_data_t0.drop(['Timepoint', 'Replicate'], axis=1)\n",
    "Novaseq_data_t10_1 = Novaseq_data_t10.drop('Timepoint', axis=1)\n",
    "\n",
    "## Rejoin the dataframe on the common columns \n",
    "Novaseq_data_wide= pd.merge(Novaseq_data_t0_1, Novaseq_data_t10_1, on=['Sample', 'Run', 'Date', 'Lane',\n",
    "       'Expected_Reads', 'Position', 'Codon', 'WT_Codon', 'Residue', 'WT_Residue'], suffixes=('_t0', '_t10'))\n",
    "\n",
    "# Reset the index\n",
    "Novaseq_data_wide = Novaseq_data_wide.reset_index(drop=True)\n",
    "\n",
    "# Reorder the columns\n",
    "Novaseq_data_wide=Novaseq_data_wide[['Sample', 'Replicate', 'Run', 'Date', 'Lane', 'Expected_Reads', \n",
    "                                     'Raw_Reads_t0', 'ng_PCR_for_library_t0', 'Merged_reads_count_R1_R2_t0', 'Percentage_merged_t0', \n",
    "                                     'Raw_Reads_t10', 'ng_PCR_for_library_t10', 'Merged_reads_count_R1_R2_t10', 'Percentage_merged_t10',\n",
    "                                     'Position', 'WT_Codon', 'Codon', 'read_counts_t0','read_counts_t10', 'log2_counts_t0', 'log2_counts_t10',\n",
    "                                     'WT_Residue','Residue']]\n",
    "\n",
    "\n",
    "# Save the file \n",
    "Novaseq_data_wide.to_csv(\"./All_run_masterfile_wide.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### I keep only the data for Lane 1, because in case of having more lanes in the run they have been concatenated\n",
    "\n",
    "Data = Novaseq_data_wide[Novaseq_data_wide['Lane'] != 2]\n",
    "\n",
    "print(\"The lenght of Novaseq Data wide \" + str(len(Data)) + \" should be equal to \" + str((69*64*3*18)))\n",
    "Data.to_csv(\"./Working_masterfile_wide.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load Working Masterfile\n",
    "Data=pd.read_csv(\"./Working_masterfile_wide.csv\")\n",
    "Novaseq_data=pd.read_csv(\"./All_run_masterfile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### I do the filtering at t0 removing all the codons that have less than 100 reads to explore the data and decide where to set the threshold\n",
    "\n",
    "Data_t0_with_less_100_counts=Data[Data.read_counts_t0 < 100]\n",
    "Data_t0_with_more_100_counts=Data[Data.read_counts_t0 >= 100]\n",
    "\n",
    "# Filter the resulting DataFrame for samples with Run == 1 and Run == 2\n",
    "Data_t0_with_less_100_counts_run1 = Data_t0_with_less_100_counts[Data_t0_with_less_100_counts.Run == 1]\n",
    "Data_t0_with_less_100_counts_run2 = Data_t0_with_less_100_counts[Data_t0_with_less_100_counts.Run == 2]\n",
    "Data_t0_with_more_100_counts_run1 = Data_t0_with_more_100_counts[Data_t0_with_more_100_counts.Run == 1]\n",
    "Data_t0_with_more_100_counts_run2 = Data_t0_with_more_100_counts[Data_t0_with_more_100_counts.Run == 2]\n",
    "\n",
    "Data_run1 = Data[Data.Run == 1]\n",
    "Data_run2 = Data[Data.Run == 2]\n",
    "\n",
    "### Print how much we lose and how much we keep \n",
    "\n",
    "print(\"For Run 1 we lose \" + str(len(Data_t0_with_less_100_counts_run1)) + \" out of \" + str(len(Data_run1)) + \" (\" + str((len(Data_t0_with_less_100_counts_run1)*100/len(Data_run1)))+ \" %) variants\")\n",
    "print(\"For Run 2 we lose \" + str(len(Data_t0_with_less_100_counts_run2)) + \" out of \" + str(len(Data_run2)) + \" (\" + str((len(Data_t0_with_less_100_counts_run2)*100/len(Data_run2)))+ \" %) variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"In Run 1 the lost reads have on average \"+str(Data_t0_with_less_100_counts_run1[\"read_counts_t0\"].mean())+\" +/- \" + str(Data_t0_with_less_100_counts_run1[\"read_counts_t0\"].std()) + \" counts per variant\" )\n",
    "print(\"In Run 2 the lost reads have on average \"+str(Data_t0_with_less_100_counts_run2[\"read_counts_t0\"].mean())+\" +/- \" + str(Data_t0_with_less_100_counts_run2[\"read_counts_t0\"].std()) + \" counts per variant\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### See also how many variants have more than 75 counts\n",
    "more75_run1=Data_t0_with_less_100_counts_run1[Data_t0_with_less_100_counts_run1.read_counts_t0 > 75]\n",
    "more75_run2=Data_t0_with_less_100_counts_run2[Data_t0_with_less_100_counts_run2.read_counts_t0 > 75]\n",
    "\n",
    "print(\"For Run 1 \" + str(len(more75_run1)) + \" out of \" + str(len(Data_t0_with_less_100_counts_run1)) + \" (\" + str((len(more75_run1)*100/len(Data_t0_with_less_100_counts_run1)))+ \" %) variants have >75 counts\")\n",
    "print(\"For Run 2 \" + str(len(more75_run2)) + \" out of \" + str(len(Data_t0_with_less_100_counts_run2)) + \" (\" + str((len(more75_run2)*100/len(Data_t0_with_less_100_counts_run2)))+ \" %) variants have >75 counts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To also help decide where to set the threshold calculate also the Log2 fold change for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate FC\n",
    "Data[\"Log2FC\"]=np.log2(Data[\"read_counts_t10\"]/Data[\"read_counts_t0\"])\n",
    "\n",
    "# Compute the absolute values\n",
    "Data['Log2FC_abs'] = Data['Log2FC'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=Data.copy()\n",
    "\n",
    "## Divide by run\n",
    "test_run1 = test[test['Run'] == 1]\n",
    "test_run2 = test[test['Run'] == 2]\n",
    "\n",
    "## Separate just one sample\n",
    "test_run1_CDS = test_run1[test_run1['Sample'] == \"CDS_\"]\n",
    "\n",
    "## Separate also for replicate because I want to do first the t0\n",
    "test_run1_CDS_rep1 = test_run1_CDS[test_run1_CDS['Replicate'] == 1]\n",
    "\n",
    "print (\"The lengt of test_run1_CDS_rep1 \" + str(len(test_run1_CDS_rep1)) + \" should be equal to \" + str(69*64))\n",
    "\n",
    "print(\"-----Continue------\")\n",
    "\n",
    "\n",
    "## Calculate all the counts that are in my sample for normalize \n",
    "total_counts_t0_CDS=test_run1_CDS_rep1[\"read_counts_t0\"].sum()\n",
    "\n",
    "# Create a new column with the abundance as the result of read_counts_t0 / total_counts_t0_CDS\n",
    "test_run1_CDS_rep1['read_abundance_t0'] = test_run1_CDS_rep1['read_counts_t0'] / total_counts_t0_CDS\n",
    "\n",
    "Sum=test_run1_CDS_rep1[\"read_abundance_t0\"].sum()\n",
    "\n",
    "\n",
    "print(\"The abundance per sample \" + str(Sum) + \" should be equal to 1\")\n",
    "print(\"-----COMPLETE-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Adding 1 to all the values in the column to avoid NAs\n",
    "\n",
    "Data[\"read_counts_t0\"]=Data[\"read_counts_t0\"]+1\n",
    "Data[\"read_counts_t10\"]=Data[\"read_counts_t10\"]+1\n",
    "\n",
    "print(\"After correction my dataset has now a minimum value of \" + str(Data[\"read_counts_t0\"].min()) + \" for t0 and \" + str(Data[\"read_counts_t10\"].min()) + \" for t10.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate read_abundance_t0 for each sample and replicate\n",
    "def calculate_read_abundance_t0(df):\n",
    "    # Calculate total counts for each combination of Sample and Replicate\n",
    "    df['total_counts_t0'] = df['read_counts_t0'].sum()\n",
    "\n",
    "    # Calculate read abundance for each combination of Sample and Replicate\n",
    "    df['read_abundance_t0'] = df['read_counts_t0'] / df['total_counts_t0']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to each unique combination of Sample and Replicate\n",
    "Data = Data.groupby(['Sample', 'Replicate', \"Run\"]).apply(calculate_read_abundance_t0)\n",
    "\n",
    "Sum=Data[\"read_abundance_t0\"].sum()\n",
    "\n",
    "print(\"The abundance per sample \" + str((Sum/3)/18) + \" should be equal to 1\")\n",
    "print(\"-----COMPLETE-----\")\n",
    "\n",
    "Data.to_csv(\"./All_run_masterfile_wide.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate also the read_abundance_t0 for each sample and replicate\n",
    "def calculate_read_abundance_t10(df):\n",
    "    # Calculate total counts for each combination of Sample and Replicate\n",
    "    df['total_counts_t10'] = df['read_counts_t10'].sum()\n",
    "\n",
    "    # Calculate read abundance for each combination of Sample and Replicate\n",
    "    df['read_abundance_t10'] = df['read_counts_t10'] / df['total_counts_t10']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to each unique combination of Sample and Replicate\n",
    "Data = Data.groupby(['Sample', 'Replicate', \"Run\"]).apply(calculate_read_abundance_t10)\n",
    "\n",
    "Sum=Data[\"read_abundance_t10\"].sum()\n",
    "\n",
    "print(\"The abundance per sample \" + str((Sum/3)/18) + \" should be equal to 1\")\n",
    "print(\"-----COMPLETE-----\")\n",
    "\n",
    "Data.to_csv(\"./All_run_masterfile_wide.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the dataframe on a threshold of a minimum number of counts at t0 to remove all those that are below the threshold to avoid bias because of the limit of foldchange detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a very low nomber at the initial time point the negative foldchange you can achieve is smaller than if you have a bigger number because the limit of detection is 0.\n",
    "If you have a variant that goes from 10 to 1 you will assume the same effect than if you go from 100 to 10, because of the limit of detection, but the first could have a lower or a higher effect but we are just not able to capture it with such a low nomber of input reads at the initial timepoint. Also random sampling effects and non-selective variation within the population can falsely enhance the fitness effect of a low count variant, because the probability of getting lost very early is higher than in high number counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### I will set a thershold of at least 100 counts for each codon to be keept in the further processing based on the previous analysis. This limit can be changed at any time.\n",
    "\n",
    "filtered_data_more_than_100counts_at_t0=Data[Data.read_counts_t0 >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define some functions for ploting the heatmaps to explore the raw data\n",
    "\n",
    "def generate_codon_heatmap(data):\n",
    "    data.index = pd.CategoricalIndex(data.index, categories=codon_order)\n",
    "    data.sort_index(level=0, inplace=True)\n",
    "    data = data.fillna(0)\n",
    "    fig, ax = plt.subplots(figsize=(22, 18))\n",
    "    sns.heatmap(np.log2(data + 1), cmap='viridis', cbar_kws={'label': 'n reads'}, vmax=10, vmin=4, ax=ax)\n",
    "    ax.set(xlabel='R67 / DfrB1 codon number', ylabel='Codon')\n",
    "    ax.xaxis.label.set_fontsize(16)\n",
    "    ax.yaxis.label.set_fontsize(16)\n",
    "    plt.xticks(rotation=90)\n",
    "    return fig, ax\n",
    "\n",
    "def add_wildtype_codons(ax, data, codon_order, wt_codons):\n",
    "    for codon in range(len(data.columns)):\n",
    "        x_pos = codon + 0.5\n",
    "        codon_n = int(list(data.columns)[codon])\n",
    "        seq = wt_codons[codon_n]\n",
    "        y_pos = list(data.index).index(seq) + 0.5\n",
    "        ax.plot(x_pos, y_pos, 'ro')\n",
    "\n",
    "        \n",
    "        \n",
    "def add_wildtype_residues(ax, data, codon_order, wt_codons, aa_order):\n",
    "    for codon in range(len(data.columns)):\n",
    "        x_pos = codon + 0.5\n",
    "        codon_n = int(list(data.columns)[codon])\n",
    "        seq = wt_codons[codon_n]\n",
    "        y_pos = aa_order.index(codontable_standard[seq]) + 0.5\n",
    "        ax.plot(x_pos, y_pos, 'ro')\n",
    "        \n",
    "# Arrange codons\n",
    "codon_order = ['TAA', 'TAG', 'TGA', # *\n",
    "            'GGA', 'GGC', 'GGG', 'GGT', # G\n",
    "            'GCA', 'GCC', 'GCG', 'GCT', # A\n",
    "            'GTA', 'GTC', 'GTG', 'GTT', # v\n",
    "            'CTA', 'CTC', 'CTG', 'CTT', 'TTA', 'TTG', # L\n",
    "            'ATA', 'ATC', 'ATT', # I\n",
    "            'ATG', # M\n",
    "            'TGC', 'TGT', # C\n",
    "            'CCA', 'CCC', 'CCG', 'CCT', # P\n",
    "            'TGG', # W\n",
    "            'TTC', 'TTT', # F\n",
    "            'TAC', 'TAT', # Y\n",
    "            'AGC', 'AGT', 'TCA', 'TCC', 'TCG', 'TCT',  # S\n",
    "            'ACA', 'ACC', 'ACG', 'ACT', # T\n",
    "            'AAC', 'AAT', # N\n",
    "            'CAA', 'CAG', # Q\n",
    "            'CAC', 'CAT', # H\n",
    "            'AAA', 'AAG', # K\n",
    "            'AGA', 'AGG', 'CGA', 'CGC', 'CGG', 'CGT', # R\n",
    "            'GAC', 'GAT', # D\n",
    "            'GAA', 'GAG' # E\n",
    "             ]       \n",
    "\n",
    "aa_order=['*', 'G','A','V', 'L', 'I', 'M', 'C', 'P', 'W', 'F', 'Y', 'S', 'T', 'N', 'Q', 'H', 'K', 'R', 'D', 'E']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Run1_figures = './Analysis/Figures/Run1/'\n",
    "Run2_figures= \"./Analysis/Figures/Run2/\"\n",
    "\n",
    "os.makedirs(Run1_figures, exist_ok = True)\n",
    "os.makedirs(Run2_figures, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Do the t0 Heatmaps for Run 1 \n",
    "Run1 = filtered_data_more_than_100counts_at_t0[filtered_data_more_than_100counts_at_t0.Run == 1]\n",
    "\n",
    "# Get unique sample names from the DataFrame\n",
    "unique_samples = Run1['Sample'].unique()\n",
    "\n",
    "# Loop through each sample and apply the code\n",
    "for sample_name in unique_samples:\n",
    "    # Prepare the data and plot the heatmaps for the current sample\n",
    "    data_pivot = Run1[(Run1.Sample == sample_name) & (Run1.Replicate == 1)]\n",
    "    data_pivot = data_pivot.pivot(index=\"Codon\", columns='Position', values='read_counts_t0')\n",
    "    \n",
    "    # Generate the heatmap\n",
    "    heatmap_fig, ax1 = generate_codon_heatmap(data_pivot)\n",
    "    \n",
    "    # Add wildtype codons\n",
    "    add_wildtype_codons(ax1, data_pivot, codon_order, wt_codons)\n",
    "    \n",
    "    # Set the title of the heatmap to the current sample name\n",
    "    ax1.set_title(sample_name)\n",
    "    \n",
    "    # Save the heatmap for each sample\n",
    "    heatmap_fig.savefig(Run1_figures+f'{sample_name}_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Do the t0 Heatmaps for Run 2 \n",
    "Run2 = filtered_data_more_than_100counts_at_t0[filtered_data_more_than_100counts_at_t0.Run == 2]\n",
    "\n",
    "# Get unique sample names from the DataFrame\n",
    "unique_samples = Run2['Sample'].unique()\n",
    "\n",
    "# Loop through each sample and apply the code\n",
    "for sample_name in unique_samples:\n",
    "    # Prepare the data and plot the heatmaps for the current sample\n",
    "    data_pivot = Run2[(Run2.Sample == sample_name) & (Run2.Replicate == 1)]\n",
    "    data_pivot = data_pivot.pivot(index=\"Codon\", columns='Position', values='read_counts_t0')\n",
    "    \n",
    "    # Generate the heatmap\n",
    "    heatmap_fig, ax1 = generate_codon_heatmap(data_pivot)\n",
    "    \n",
    "    # Add wildtype codons\n",
    "    add_wildtype_codons(ax1, data_pivot, codon_order, wt_codons)\n",
    "    \n",
    "    # Set the title of the heatmap to the current sample name\n",
    "    ax1.set_title(sample_name)\n",
    "    \n",
    "    # Save the heatmap for each sample\n",
    "    heatmap_fig.savefig(Run2_figures+f'{sample_name}_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save the filtered dataframe \n",
    "\n",
    "filtered_data_more_than_100counts_at_t0.to_csv(\"./Filtered_100at0_wide.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CALCULATE SELECTION COEFFICIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the data\n",
    "filtered_data = pd.read_csv(\"./Filtered_100at0_wide.csv\")\n",
    "\n",
    "# Keep all the rows that on Codon are different from TAG. I will filter out this codon. \n",
    "### Stop codon TAG was removed from all datasets because it has been shown to have a lower termination efficiency than the other stop codons in E.coli       \n",
    "filtered_data = filtered_data[filtered_data.Codon != 'TAG']\n",
    "\n",
    "\n",
    "filtered_data.to_csv(sel_coff_prefix + \"Filtered_100at0_wide_no_TAG.csv\", sep = ',', index=False)\n",
    "filtered_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get the median for the WT\n",
    "\n",
    "## Keep only the rows for those the Codon is the WT_Codon\n",
    "\n",
    "WT_medians = filtered_data[filtered_data.Codon == filtered_data.WT_Codon]\n",
    "\n",
    "WT_medians=WT_medians[['Sample', 'Replicate', 'Run', 'Position', 'WT_Codon', 'Codon', \n",
    "                       'read_counts_t0','log2_counts_t0', 'read_abundance_t0',\n",
    "                       'read_counts_t10', 'log2_counts_t10', 'read_abundance_t10',  \n",
    "                       'WT_Residue', 'Residue']]\n",
    "WT_medians=WT_medians.sort_values(by=['Position'])\n",
    "\n",
    "WT_medians\n",
    "\n",
    "WT_medians = WT_medians.groupby(['Sample', 'Replicate', \"Run\"]).agg({'read_abundance_t0': ['median'],'read_abundance_t10': ['median']})\n",
    "\n",
    "WT_medians.reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "WT_medians.columns = pd.MultiIndex.from_tuples([('read_abundance_t0', 'WT_median_t0'),('read_abundance_t10', 'WT_median_t10'),])\n",
    "\n",
    "WT_medians.columns = WT_medians.columns.droplevel()\n",
    "\n",
    "filtered_data_WTmedian=pd.merge(filtered_data, WT_medians, on=['Sample', 'Replicate', 'Run'])\n",
    "\n",
    "# complete_codons_with_WT_no_TAG_WTmedian.columns\n",
    "\n",
    "print(\"The length of filtered_data_WTmedian \" + str(len(filtered_data_WTmedian)) + \" should be equal to \" + str(len(filtered_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Divide the normalized number of reads by the median of WT reads for each position for t0 amd t10\n",
    "filtered_data_WTmedian[\"mut_wt_ratio_t0\"]=filtered_data_WTmedian[\"read_abundance_t0\"].div(filtered_data_WTmedian[\"WT_median_t0\"])\n",
    "filtered_data_WTmedian[\"mut_wt_ratio_t10\"]=filtered_data_WTmedian[\"read_abundance_t10\"].div(filtered_data_WTmedian[\"WT_median_t10\"])\n",
    "\n",
    "\n",
    "## Add a column for the timepoint \n",
    "filtered_data_WTmedian[\"Timepoint\"]=10\n",
    "\n",
    "## Calculate the selection coefficient\n",
    "filtered_data_WTmedian[\"Sel_coeff\"]=(np.log2(filtered_data_WTmedian[\"mut_wt_ratio_t10\"]/filtered_data_WTmedian[\"mut_wt_ratio_t0\"]))/filtered_data_WTmedian[\"Timepoint\"]\n",
    "\n",
    "print(\"The length of filtered_data_WTmedian \" + str(len(filtered_data_WTmedian)) + \" should be equal to \" + str(len(filtered_data)))\n",
    "\n",
    "## Save the new file with the selection coefficients\n",
    "filtered_data_WTmedian.to_csv(sel_coff_prefix + \"Filtered_100at0_wide_no_TAG_sel_coeff.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_data_WTmedian = pd.read_csv(\"./Analysis/Sel_coff/Filtered_100at0_wide_no_TAG_sel_coeff.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
