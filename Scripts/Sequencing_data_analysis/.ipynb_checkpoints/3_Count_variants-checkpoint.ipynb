{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 003_sequencing_data_processing: \n",
    "\n",
    "This script finds the mutations in the alignment and count the numers of times each mutation shows up in each library.\n",
    "\n",
    "This script assumes the following folder layout\n",
    "- Home folder\n",
    "    - Scripts folder with this script\n",
    "    - Data/Analysis_NovaSeq/amplicon_sequences folder with the reference amplicon fasta file\n",
    "    - Data/Analysis_NovaSeq/aggregate_reads folder for aggregated sequences from the done in the previous step\n",
    "    - Data/Analysis_NovaSeq/amplicon_align folder for aligned sequences from the done in the previous step\n",
    "    - Data/Analysis_NovaSeq folder for the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import modules and check versions\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.__name__, pd.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__name__, np.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "print(matplotlib.__name__, matplotlib.__version__)\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "print(scipy.__name__, scipy.__version__)\n",
    "\n",
    "import re\n",
    "print(re.__name__, re.__version__)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define some helper functions that will help us for the further analysis\n",
    "def reverse_complement(dna):\n",
    "    \"\"\" function that reverse complements DNA\n",
    "    dna: input dna sequence\n",
    "    \"\"\"\n",
    "    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "    return ''.join([complement[base] for base in dna[::-1]])\n",
    "\n",
    "def get_dict_of_seq(fasta_file):\n",
    "    \"\"\" function that converts a fasta file to a dictionnary of sequences\n",
    "    fasta_file: the input fasta file\n",
    "    \"\"\"\n",
    "    \n",
    "    file_fasta_dict = {}\n",
    "    # output dict of imported seqs\n",
    "    \n",
    "    with open(fasta_file, 'r') as fasta:    \n",
    "        for line in fasta:\n",
    "            # loops through the file\n",
    "\n",
    "            if line.startswith('>') == True:\n",
    "                seq_info = line.strip('>').strip('\\n').split('\\t')[0]\n",
    "                file_fasta_dict[seq_info] = ''\n",
    "                # checks if seq header\n",
    "\n",
    "            else:\n",
    "                file_fasta_dict[seq_info] += line.strip('\\n')\n",
    "                # If not, append nt to current seq\n",
    "                \n",
    "    return file_fasta_dict\n",
    "\n",
    "codontable_standard = {\n",
    "    'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',\n",
    "    'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "    'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',\n",
    "    'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',\n",
    "    'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "    'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "    'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "    'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "    'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',\n",
    "    'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "    'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',\n",
    "    'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "    'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "    'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',\n",
    "    'TAC':'Y', 'TAT':'Y', 'TAA':'*', 'TAG':'*',\n",
    "    'TGC':'C', 'TGT':'C', 'TGA':'*', 'TGG':'W',\n",
    "    }\n",
    "\n",
    "def convert_dict_to_aa(codon_dict):\n",
    "    \n",
    "    aa_dict_of_dicts = {}\n",
    "    \n",
    "    for pos in list(codon_dict.keys()):\n",
    "        \n",
    "        aa_dict_of_dicts[pos] = {}\n",
    "        \n",
    "        for codon in list(codon_dict[pos].keys()):            \n",
    "            \n",
    "            if np.isnan(codon_dict[pos][codon]):\n",
    "                continue\n",
    "            \n",
    "            aa = codontable_standard[codon]\n",
    "            \n",
    "            if aa in list(aa_dict_of_dicts[pos].keys()):           \n",
    "                aa_dict_of_dicts[pos][aa] += codon_dict[pos][codon]\n",
    "                \n",
    "            else:\n",
    "                aa_dict_of_dicts[pos][aa] = codon_dict[pos][codon]\n",
    "                \n",
    "    return aa_dict_of_dicts\n",
    "\n",
    "os.chdir(\"/path/Novaseq_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define the reference sequence \n",
    "\n",
    "path_to_amplicons = './Data/uni_amplicon_sequences/Novaseq_R67.fasta'\n",
    "\n",
    "amplicon_info_dict = get_dict_of_seq(path_to_amplicons)\n",
    "\n",
    "amplicon_length_dict = {}\n",
    "\n",
    "amplicon_dict = {}\n",
    "\n",
    "for amplicon in amplicon_info_dict:\n",
    "    \n",
    "    amplicon_name = amplicon.split('|')[0]\n",
    "\n",
    "    amplicon_dict[amplicon_name] = amplicon_info_dict[amplicon]\n",
    "    \n",
    "    amplicon_fasta_path = './Data/Analysis_Novaseq/amplicon_sequences/'+amplicon_name+'.fasta'\n",
    "    \n",
    "    with open(amplicon_fasta_path, 'w') as dest:\n",
    "        \n",
    "        seq_ID = '>'+amplicon_name+'\\n'\n",
    "        seq = amplicon_dict[amplicon_name]+'\\n'\n",
    "        \n",
    "        dest.write(seq_ID)\n",
    "        dest.write(seq)\n",
    "    \n",
    "\n",
    "print (amplicon_info_dict.keys())\n",
    "print (amplicon_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions to find mutations and count the variants at each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_needle_output(needle_align_path):\n",
    "    \n",
    "    n_aligns = 0\n",
    "    align_seqs_dict = {}       \n",
    "   \n",
    "    with open(needle_align_path, 'r') as source:\n",
    "\n",
    "        current_align = ''\n",
    "        current_qseq = ''\n",
    "        current_sseq = ''\n",
    "        qseq_done = 0\n",
    "\n",
    "        for line in source:\n",
    "\n",
    "            if line.startswith('>>>') == True:\n",
    "\n",
    "                n_aligns +=1\n",
    "                align_name = line.strip('>>>')\n",
    "\n",
    "                if n_aligns != 1:\n",
    "\n",
    "                    align_seqs_dict[current_align] = [current_qseq, current_sseq]\n",
    "                    current_align = align_name\n",
    "                    current_qseq = ''\n",
    "                    current_sseq = ''\n",
    "                    qseq_done = 0\n",
    "\n",
    "                else:\n",
    "\n",
    "                    current_align = align_name\n",
    "\n",
    "            elif line.startswith(';') == False and line.startswith('>') == False and line.startswith('\\n') == False and line.startswith('#') == False:\n",
    "\n",
    "                if qseq_done == 1:\n",
    "                    current_sseq += line.strip('\\n').upper()\n",
    "\n",
    "                else:\n",
    "                    current_qseq += line.strip('\\n').upper()\n",
    "\n",
    "            elif line.startswith('#--') == True:\n",
    "\n",
    "                align_seqs_dict[align_name] = [current_qseq, current_sseq]\n",
    "\n",
    "            else:\n",
    "\n",
    "                if qseq_done == 0 and current_qseq != '':\n",
    "                    qseq_done =1            \n",
    "                             \n",
    "    return align_seqs_dict, n_aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_mutations(path, ref_orf):\n",
    "    \n",
    "    allele_dict = {}\n",
    "\n",
    "    align_dict, align_count = parse_needle_output(path)\n",
    "\n",
    "    for entry in list(align_dict.keys()):\n",
    "\n",
    "        read_var_list = []\n",
    "\n",
    "        query_seq = align_dict[entry][1]\n",
    "        # aligned prot sequence of the strain\n",
    "\n",
    "        align_ref = align_dict[entry][0]\n",
    "        # aligned prot sequence of the reference\n",
    "\n",
    "        gap_adjust = 0\n",
    "        # value used to adjust the protein sequence index for the presence of insertions in the strain sequence vs the \n",
    "        # reference strain\n",
    "\n",
    "        backtrack_adjust = 0\n",
    "\n",
    "        temp_var = None\n",
    "        # temporary variable to hold the sequence of an insertion or deletion as a string. When the gap ends, annotation \n",
    "        # will be added to strain_var_list\n",
    "\n",
    "        indel_start = 0\n",
    "        # position start of the indel annotation in the reference sequence, with adjustment for gap presence\n",
    "\n",
    "        ref_seq_no_gaps = align_ref.replace('-','')\n",
    "        # Make a copy of the reference sequence without gaps \n",
    "        \n",
    "        align_start = (amplicon_dict[ref_orf].upper().index(ref_seq_no_gaps))+1\n",
    "        # Look for the starting position of the gapless part of the reference sequence\n",
    "        # This helps remove gaps at the start and the end of the alignment\n",
    "        \n",
    "        query_seq_no_gaps = len(query_seq.replace('-',''))\n",
    "        # Make a copy of the query sequence without gaps\n",
    "        \n",
    "        for nt in range(0, len(align_ref)):\n",
    "            # iterates through the entire alignment of the strain prot sequence\n",
    "\n",
    "            if query_seq[nt] == '-':\n",
    "                # detect a deletion variant\n",
    "\n",
    "                # logic for indel detection/annotation:\n",
    "                #\n",
    "                # suppose we have this alignment  \n",
    "                #\n",
    "                # 1 2 3 4 5 6 7 8 9\n",
    "                # A T - - A A A T G    strain variant: del gaps are indexed because the aa index is based on reference\n",
    "                # A T K P A - - T G\n",
    "                # 1 2 3 4 5     6 7    reference: insert gaps not indexed because aa positions do (actually don't?) exist in reference\n",
    "                #\n",
    "                # following this logic, every time an insertion is detected and annotated, the gap_adjust value is \n",
    "                # incremented by the length of the gap and used to adjust the variant mapping to make it match the \n",
    "                # reference index values. The indel aa postion is the first residue detected as part of the indel\n",
    "\n",
    "\n",
    "                if indel_start == 0:\n",
    "                    # checks if the character is the start or the continuation of a gap in the alignment\n",
    "\n",
    "                    temp_var = 'del'+ align_ref[nt]\n",
    "                    indel_start = (nt+1-gap_adjust)\n",
    "                    # if it is, starts a new annotation entry with a start position compensated for previous insertions\n",
    "                    # (if any)\n",
    "\n",
    "                    backtrack_adjust += 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    temp_var += align_ref[nt]\n",
    "                    # if it is not, adds the following aa to the deletion annotation\n",
    "\n",
    "                    backtrack_adjust += 1\n",
    "\n",
    "\n",
    "            elif align_ref[nt] == '-':\n",
    "                # detects an insertion variant\n",
    "\n",
    "                if indel_start == 0:\n",
    "                    # checks if the character is the start or the continuation of a gap in the alignment\n",
    "\n",
    "                    temp_var = 'ins'+ query_seq[nt]\n",
    "\n",
    "                    indel_start = (nt+1-gap_adjust)\n",
    "                    # if it is, starts a new annotation entry with a start position compensated for previous insertions\n",
    "                    # (if any)\n",
    "\n",
    "                    gap_adjust += 1\n",
    "                    # increments the gap adjust for the this added aa in the strain sequence                   \n",
    "\n",
    "                else:\n",
    "\n",
    "                    temp_var += query_seq[nt]\n",
    "                    # if it is not, adds the following aa to the insertion annotation\n",
    "\n",
    "                    gap_adjust += 1\n",
    "                    # increments the gap adjust for the this added aa in the strain sequence\n",
    "\n",
    "            elif query_seq[nt] != align_ref[nt]:\n",
    "                # detects a mismatch between the strain sequence and the reference\n",
    "\n",
    "                variant = align_ref[nt]+'|'+str((nt+1-gap_adjust))+'|'+query_seq[nt]\n",
    "                read_var_list.append(variant)\n",
    "                # creates an annotation for the strain-reference aa mismatch and appends it to the list of \n",
    "                # annotations\n",
    "\n",
    "            else:\n",
    "\n",
    "                 if indel_start != 0:\n",
    "                    # detects if there is currently an open gap entry. If there is, then the detected mismatch means \n",
    "                    # that it has now concluded\n",
    "\n",
    "                    read_var_list.append(str((indel_start))+temp_var)\n",
    "                    temp_var = None\n",
    "                    indel_start = 0\n",
    "                    # adds the indel annotation to the strain variant list and resets temporary variables for the next \n",
    "                    # indel entry\n",
    "\n",
    "        if len(read_var_list)<25:  \n",
    "            allele_dict[entry] = read_var_list, align_start\n",
    "                           \n",
    "    return allele_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_variant_count_1(mutation_set, ref_seq, frag_start, codon_start, n_aa):\n",
    "    \n",
    "    variant_abundance_dict ={}\n",
    "    variants = list(mutation_set.keys())\n",
    "    codon_groups = {}\n",
    "    codon = 0\n",
    "    wt_count =0\n",
    "    valid_seq=0\n",
    "    \n",
    "    # Calculate the end of the fragment with the frag start and the number of residues\n",
    "    \n",
    "    frag_end = frag_start + ((n_aa -1 )* 3)\n",
    "    \n",
    "    for nt in range(0, (n_aa - 1)*3):\n",
    "        pos = nt + frag_start\n",
    "        \n",
    "        if nt % 3 == 0:\n",
    "            codon += 1\n",
    "            \n",
    "        codon_groups[pos] = codon\n",
    "        \n",
    "        variant_abundance_dict[codon] = {}\n",
    "        \n",
    "    wt_codons = {}\n",
    "    \n",
    "    ref = amplicon_dict[ref_seq].upper()\n",
    "    \n",
    "    # Set the abundance of WT codons to not a number (nan) since they would be overrepresented\n",
    "    \n",
    "    for aa in range(0, n_aa - 1): \n",
    "        \n",
    "        offset = frag_start - 1\n",
    "        start = offset+(aa*3)\n",
    "        wt_codon=ref[start:(start+3)]\n",
    "        wt_codons[(aa+codon_start)] = wt_codon\n",
    "        variant_abundance_dict[aa+codon_start][wt_codon]=np.nan\n",
    "        \n",
    "    for variant in variants:\n",
    "        \n",
    "        var_info = variant.split(',')\n",
    "        var_count =int(var_info[1].split(';')[1].strip('size='))      \n",
    "        mut_list = mutation_set[variant][0]\n",
    "        filtered_list = []\n",
    "\n",
    "        \n",
    "        # Only keep variants that appeared in more than 20 reads\n",
    "        if var_count>=20:\n",
    "        \n",
    "            for mutation in mut_list:\n",
    "\n",
    "                if 'del' in mutation:\n",
    "                    mut_info = mutation.split('del')\n",
    "                    mut_pos = int(mut_info[0])\n",
    "                    \n",
    "                    if mut_pos == 1:\n",
    "                        mut_list.remove(mutation)\n",
    "        \n",
    "            # If not an indel\n",
    "            if 'ins' not in str(mut_list) and 'del' not in str(mut_list):\n",
    "                \n",
    "                if len(mut_list) ==0:\n",
    "                    wt_count += var_count\n",
    "                    \n",
    "                else:\n",
    "                    mut_nt_list = []\n",
    "                    out_list = []\n",
    "                    \n",
    "                    for mutation in mut_list:\n",
    "                        \n",
    "                        mut_pos = int(mutation.split('|')[1])\n",
    "                        \n",
    "                        if mut_pos >= frag_start and mut_pos <= frag_end:\n",
    "                        \n",
    "                            # mut_nt_list contains the list of mutated positions inside the coding sequence\n",
    "                            mut_nt_list.append(codon_groups[mut_pos])\n",
    "                            \n",
    "                        else:\n",
    "                            # out_list contains the list of mutated positions outside the coding sequence\n",
    "                            out_list.append(mut_pos)\n",
    "                        \n",
    "                    if len(set(mut_nt_list)) == 1:\n",
    "                        valid_seq+=var_count\n",
    "                        \n",
    "                        codon = int(list(set(mut_nt_list))[0])\n",
    "                        \n",
    "                        wt_seq = wt_codons[codon]\n",
    "                                                                      \n",
    "                        new_seq = [x for x in wt_seq]\n",
    "                        \n",
    "                        for mutation in mut_list:\n",
    "                            mut_pos = int(mutation.split('|')[1])\n",
    "                            mutation = mutation.split('|')[2]\n",
    "                            \n",
    "                            codon_pos = (mut_pos-1)%3\n",
    "                            \n",
    "                            new_seq[codon_pos] = mutation\n",
    "                            \n",
    "                        new_codon = ''.join(new_seq)\n",
    "                        \n",
    "                        if new_codon in list(variant_abundance_dict[codon].keys()):\n",
    "\n",
    "                            variant_abundance_dict[codon][new_codon]+=var_count\n",
    "                            \n",
    "                        else:\n",
    "                            variant_abundance_dict[codon][new_codon]=var_count\n",
    "  \n",
    "                    # This is for mutations outside of the coding sequence\n",
    "                    elif len(set(mut_nt_list)) == 0 and len(out_list)>=1:\n",
    "                        wt_count+=var_count            \n",
    "            \n",
    "        ## For low abundance variants            \n",
    "        elif var_count < 20:\n",
    "\n",
    "            for mutation in mut_list:\n",
    "\n",
    "                if 'del' in mutation:\n",
    "                    mut_info = mutation.split('del')\n",
    "                    mut_pos = int(mut_info[0])\n",
    "\n",
    "                    if mut_pos == 1:\n",
    "                        mut_list.remove(mutation)\n",
    "                  \n",
    "            if len(mut_list) <=3 and 'ins' not in str(mut_list) and 'del' not in str(mut_list):\n",
    "\n",
    "                if len(mut_list) ==0:\n",
    "                    wt_count += var_count\n",
    "\n",
    "                else:\n",
    "                    #print(mut_list)\n",
    "                    mut_nt_list = []\n",
    "\n",
    "                    out_list = []\n",
    "\n",
    "                    for mutation in mut_list:\n",
    "\n",
    "                        mut_pos = int(mutation.split('|')[1])\n",
    "\n",
    "                        if mut_pos >= frag_start and mut_pos <= frag_end:\n",
    "\n",
    "                            mut_nt_list.append(codon_groups[mut_pos])\n",
    "\n",
    "                        else:\n",
    "                            out_list.append(mut_pos)\n",
    "\n",
    "                    if len(set(mut_nt_list)) == 1:\n",
    "                        valid_seq+=var_count\n",
    "\n",
    "                        codon = int(list(set(mut_nt_list))[0])\n",
    "\n",
    "                        wt_seq = wt_codons[codon]\n",
    "\n",
    "                        new_seq = [x for x in wt_seq]\n",
    "\n",
    "                        for mutation in mut_list:\n",
    "                            mut_pos = int(mutation.split('|')[1])\n",
    "                            mutation = mutation.split('|')[2]\n",
    "\n",
    "                            codon_pos = (mut_pos-1)%3\n",
    "\n",
    "                            new_seq[codon_pos] = mutation\n",
    "\n",
    "                        new_codon = ''.join(new_seq)\n",
    "                        \n",
    "                        if new_codon in list(variant_abundance_dict[codon].keys()):\n",
    "                            \n",
    "                            variant_abundance_dict[codon][new_codon]+=var_count\n",
    "  \n",
    "                        else:\n",
    "                            variant_abundance_dict[codon][new_codon]=var_count\n",
    "                    \n",
    "    return variant_abundance_dict, wt_count, wt_codons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new function to count the variants normalizing by the total number of reads in each library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_timepoint_fraction_df(needle_file, Sample_ID):\n",
    "    \n",
    "    # Find mutations (this function parses the needle file)\n",
    "    pool_1_muts = find_mutations(needle_file, 'Novaseq_R67_bacteria')\n",
    "    \n",
    "    frag_1_dict = get_variant_count_1(pool_1_muts, 'Novaseq_R67_bacteria', 1, 1, 71)\n",
    "    mut_dict = frag_1_dict[0]\n",
    "    wt_count = frag_1_dict[1]\n",
    "        \n",
    "    variant_df = pd.DataFrame(mut_dict)\n",
    "    \n",
    "    array_size=len(variant_df.to_numpy().flatten())\n",
    "    \n",
    "    # Calculate the total number of reads before adding the WT count to the matrix\n",
    "    read_total = variant_df.sum().sum()+wt_count+array_size\n",
    "    print(Sample_ID, read_total, wt_count, array_size)\n",
    "    \n",
    "    # Extract the dictionary with the WT codons\n",
    "    wt_codons = frag1_dict[2]\n",
    "    \n",
    "    # Add a loop to fill in the WT codons\n",
    "    for position, wt_codon in wt_codons.items():\n",
    "        mut_dict[position][wt_codon] = wt_count\n",
    "    \n",
    "    # Update variant_df\n",
    "    variant_df = pd.DataFrame(mut_dict)\n",
    "    variant_df_no_NaN = variant_df.fillna(0)\n",
    "    \n",
    "    variant_df_no_NaN = variant_df_no_NaN + 1\n",
    "        \n",
    "    read_fraction_df = variant_df_no_NaN/(read_total)\n",
    "    print(read_total, 1/read_total)\n",
    "    \n",
    "    read_fraction_df.rename_axis(read_total, inplace=True)\n",
    "    \n",
    "    df_out_path='./Data/Analysis_Novaseq/read_abundances/Codons/'+str(Sample_ID)+'_read_frac.csv'\n",
    "    \n",
    "    read_fraction_df.to_csv(df_out_path, sep=',')\n",
    "    \n",
    "    # Convert dataframe to the residue level\n",
    "    aa_frag_1 = convert_dict_to_aa(mut_dict)\n",
    "    variant_df_aa = pd.DataFrame(aa_frag_1)\n",
    "    \n",
    "    # Get proportion of reads at the residue level\n",
    "    array_size=len(variant_df_aa.to_numpy().flatten())\n",
    "    read_total = variant_df_aa.sum().sum() + wt_count + array_size\n",
    "    \n",
    "    print(Sample_ID, read_total, wt_count, array_size)\n",
    "    \n",
    "    variant_df_aa_no_NaN = variant_df_aa.fillna(0)\n",
    "    variant_df_aa_no_NaN = variant_df_aa_no_NaN + 1\n",
    "    \n",
    "    # Normalize by the total of reads\n",
    "    read_fraction_df_aa = variant_df_aa_no_NaN/(read_total)\n",
    "    print(read_total, 1/read_total)\n",
    "    \n",
    "    read_fraction_df_aa.rename_axis(read_total, inplace=True)\n",
    "    \n",
    "    # Save file\n",
    "    df_out_path='./Data/Analysis_Novaseq/read_abundances/Residues/'+str(Sample_ID)+'_read_frac.csv'\n",
    "    read_fraction_df_aa.to_csv(df_out_path, sep=',')\n",
    "    \n",
    "    # variant_df has the raw counts for each codon\n",
    "    # read_fraction_df has counts normalized by the total of reads\n",
    "    return read_fraction_df, variant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('./Data/Analysis_Novaseq/read_abundances/', exist_ok = True)\n",
    "os.makedirs('./Data/Analysis_Novaseq/read_abundances/Codons/', exist_ok = True)\n",
    "os.makedirs('./Data/Analysis_Novaseq/read_abundances/Residues/', exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The loop to calculate read proportions in each sample\n",
    "\n",
    "for file in dict_path_to_align_files.values():\n",
    "    needle_file=file\n",
    "\n",
    "    # Set the sample name\n",
    "    test = file.rstrip(\".needle\")\n",
    "    sample_name=test.lstrip(\"./Data/Analysis_Novaseq/amplicon_align/Novaseq_R67_bacteria/\")\n",
    "    \n",
    "    print(sample_name)\n",
    "    \n",
    "    read_fraction_df, variant_df = get_timepoint_fraction_df(needle_file, sample_name)\n",
    "        \n",
    "    print('-----done----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a master dataframe that contains all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the genetic code and get the sample information from the excel sheet\n",
    "codons = [\n",
    "  'ATA', 'ATC', 'ATT', 'ATG',\n",
    "  'ACA', 'ACC', 'ACG', 'ACT',\n",
    "  'AAC', 'AAT', 'AAA', 'AAG',\n",
    "  'AGC', 'AGT', 'AGA', 'AGG',\n",
    "  'CTA', 'CTC', 'CTG', 'CTT',\n",
    "  'CCA', 'CCC', 'CCG', 'CCT',\n",
    "  'CAC', 'CAT', 'CAA', 'CAG',\n",
    "  'CGA', 'CGC', 'CGG', 'CGT',\n",
    "  'GTA', 'GTC', 'GTG', 'GTT',\n",
    "  'GCA', 'GCC', 'GCG', 'GCT',\n",
    "  'GAC', 'GAT', 'GAA', 'GAG',\n",
    "  'GGA', 'GGC', 'GGG', 'GGT',\n",
    "  'TCA', 'TCC', 'TCG', 'TCT',\n",
    "  'TTC', 'TTT', 'TTA', 'TTG',\n",
    "  'TAC', 'TAT', 'TAA', 'TAG',\n",
    "  'TGC', 'TGT', 'TGA', 'TGG'\n",
    "]\n",
    "\n",
    "residues = [\n",
    "  'I', 'I', 'I', 'M',\n",
    "  'T', 'T', 'T', 'T',\n",
    "  'N', 'N', 'K', 'K',\n",
    "  'S', 'S', 'R', 'R',\n",
    "  'L', 'L', 'L', 'L',\n",
    "  'P', 'P', 'P', 'P',\n",
    "  'H', 'H', 'Q', 'Q',\n",
    "  'R', 'R', 'R', 'R',\n",
    "  'V', 'V', 'V', 'V',\n",
    "  'A', 'A', 'A', 'A',\n",
    "  'D', 'D', 'E', 'E',\n",
    "  'G', 'G', 'G', 'G',\n",
    "  'S', 'S', 'S', 'S',\n",
    "  'F', 'F', 'L', 'L',\n",
    "  'Y', 'Y', '*', '*',\n",
    "  'C', 'C', '*', 'W'\n",
    "]\n",
    "\n",
    "gen_code = {'Codon': codons, 'Residue': residues}\n",
    "genetic_code = pd.DataFrame(data=gen_code)\n",
    "\n",
    "\n",
    "### Get the sample information from the excel sheet\n",
    "metada= pd.read_excel('./Data/Novaseq_Sample_Info_df.xlsx', index_col=1) \n",
    "metada=metada.drop(columns=['Sample'])\n",
    "metada.head()\n",
    "\n",
    "### Create a directoty to save the selection coeffitient files\n",
    "\n",
    "sel_coff_prefix = './Data/Analysis_Novaseq/Sel_coff/'\n",
    "os.makedirs(sel_coff_prefix, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make a dictionary to identify the different files\n",
    "\n",
    "path_to_codon_read_abundances = \"./Data/Analysis_Novaseq/read_abundances/Codons/\"\n",
    "\n",
    "dict_path_to_codons_read_abundances = {}\n",
    "\n",
    "path_to_residues_read_abundances = \"./Data/Analysis_Novaseq/read_abundances/Residues/\"\n",
    "\n",
    "dict_path_to_residues_read_abundances = {}\n",
    "\n",
    "\n",
    "for filename in os.listdir(path_to_codon_read_abundances):\n",
    "    \n",
    "    f = os.path.join(path_to_codon_read_abundances, filename)\n",
    "    \n",
    "    f1 = f.rstrip(\"_read_frac.csv\")\n",
    "    ### I want the sample name to be the same as the Sample.1 in the excel dataframe\n",
    "    \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/read_abundances/\")\n",
    "    \n",
    "    sample_name2=sample_name.lstrip(\"Codons\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/\")\n",
    "    \n",
    "#     print(sample_name3)\n",
    "    \n",
    "    dict_path_to_codons_read_abundances[sample_name3] = f\n",
    "\n",
    "dict_path_to_codons_read_abundances\n",
    "\n",
    "for filename in os.listdir(path_to_residues_read_abundances):\n",
    "    \n",
    "    f = os.path.join(path_to_residues_read_abundances, filename)\n",
    "    \n",
    "    f1 = f.rstrip(\"_read_frac.csv\")\n",
    "       \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/read_abundances/\")\n",
    "    \n",
    "    sample_name2=sample_name.lstrip(\"Residues\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/\")\n",
    "    \n",
    "    dict_path_to_residues_read_abundances[sample_name3] = f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make also a dictionary to identify the different files with the counts instead of the fractions\n",
    "\n",
    "path_to_codon_counts = \"./Data/Analysis_Novaseq/aggregate_dataframes/Codons/\"\n",
    "\n",
    "dict_path_to_codons_counts = {}\n",
    "\n",
    "path_to_residues_counts = \"./Data/Analysis_Novaseq/aggregate_dataframes/Residues/\"\n",
    "\n",
    "dict_path_to_residues_counts = {}\n",
    "\n",
    "\n",
    "for filename in os.listdir(path_to_codon_counts):\n",
    "    \n",
    "    f = os.path.join(path_to_codon_counts, filename)\n",
    "    \n",
    "    f1 = f.rstrip(\"_codons.txt\")\n",
    "    ### I want the sample name to be the same as the Sample.1 in the excel dataframe\n",
    "    \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/aggregate_dataframes/\")\n",
    "    \n",
    "    sample_name2=sample_name.lstrip(\"Codons\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/Novaseq_R67_bacteria_\")\n",
    "    \n",
    "    dict_path_to_codons_counts[sample_name3] = f\n",
    "\n",
    "# dict_path_to_codons_counts\n",
    "    \n",
    "for filename in os.listdir(path_to_residues_counts):\n",
    "    \n",
    "    f = os.path.join(path_to_residues_counts, filename)\n",
    "    \n",
    "    f1 = f.rstrip(\"_residues.txt\")\n",
    "       \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/aggregate_dataframes/\")\n",
    "    \n",
    "    sample_name2=sample_name.lstrip(\"Residues\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/Novaseq_R67_bacteria_\")\n",
    "    \n",
    "    dict_path_to_residues_counts[sample_name3] = f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get the codon abundances for each sample and format the dataframe into the correct \n",
    "\n",
    "codon_abundances = './Data/Analysis_Novaseq/Sel_coff/Codons/'\n",
    "os.makedirs(codon_abundances, exist_ok = True)\n",
    "\n",
    "all_codon_data=pd.DataFrame()\n",
    "\n",
    "for file in dict_path_to_codons_read_abundances.values():\n",
    "    \n",
    "    ### Import the file\n",
    "    codon_df=pd.read_csv(file)\n",
    "    \n",
    "    ### Change the first column name\n",
    "    codon_df.rename(columns = {list(codon_df)[0]:'Codon'}, inplace=True)\n",
    "    \n",
    "    ### Unpivot the codon_df to a longer dataframe in wich each row is the read_abundance for each codon at each position\n",
    "    new_codon= pd.melt(codon_df, id_vars=\"Codon\")\n",
    "    new_codon.rename({'variable': 'Position', 'value': 'read_abundance'}, axis=1, inplace=True)\n",
    "\n",
    "    ### Get the sample ID as it is in metadata\n",
    "    f1 = file.rstrip(\"_read_frac.csv\")\n",
    "        \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/read_abundances/\")\n",
    "         \n",
    "    sample_name2=sample_name.lstrip(\"Codons\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/\")\n",
    "\n",
    "    \n",
    "    ### Add a new column with the sample ID to the codon df\n",
    "    \n",
    "    new_codon['Sample.1'] = sample_name3\n",
    "    \n",
    "    \n",
    "    ### Save the file\n",
    "    new_codon.to_csv(codon_abundances + sample_name3 + '_codons.csv', sep = ',', index=False)\n",
    "\n",
    "    ### I need to concatenate all the dataframes into the same master dataframe \"all_codon_data\"\n",
    "    all_codon_data=all_codon_data.append(new_codon,ignore_index=True)\n",
    "    \n",
    "### Save the file master file \n",
    "all_codon_data.to_csv(sel_coff_prefix + \"All_codon_abundance.csv\", sep = ',', index=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get the residues abundances for each sample and format the dataframe into the correct format\n",
    "\n",
    "Residues_abundances = './Data/Analysis_Novaseq/Sel_coff/Residues/'\n",
    "os.makedirs(Residues_abundances, exist_ok = True)\n",
    "\n",
    "all_residues_data=pd.DataFrame()\n",
    "\n",
    "for file in dict_path_to_residues_read_abundances.values():\n",
    "    \n",
    "    ### Import the file\n",
    "    residues_df=pd.read_csv(file)\n",
    "    \n",
    "    ### Change the first column name\n",
    "    residues_df.rename(columns = {list(residues_df)[0]:'Residue'}, inplace=True)\n",
    "    \n",
    "    ### Unpivot the residues_df to a longer dataframe in wich each row is the read_abundance for each codon at each position\n",
    "    new_residue= pd.melt(residues_df, id_vars=\"Residue\")\n",
    "    new_residue.rename({'variable': 'Position', 'value': 'read_abundance'}, axis=1, inplace=True)\n",
    "\n",
    "    ### Get the sample ID as it is in metadata\n",
    "    f1 = file.rstrip(\"_read_frac.csv\")\n",
    "        \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/read_abundances/\")\n",
    "         \n",
    "    sample_name2=sample_name.lstrip(\"Residues\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/\")\n",
    "    \n",
    "    ### Add a new column with the sample ID to the codon df\n",
    "    \n",
    "    new_residue['Sample.1'] = sample_name3\n",
    "    \n",
    "    \n",
    "    ### Save the file\n",
    "    new_residue.to_csv(Residues_abundances + sample_name3 + '_residues.csv', sep = ',', index=False)\n",
    "\n",
    "    ### I need to concatenate all the dataframes into the same master dataframe \"all_codon_data\"\n",
    "    all_residues_data=all_residues_data.append(new_residue,ignore_index=True)\n",
    "    \n",
    "### Save the file master file \n",
    "all_residues_data.to_csv(sel_coff_prefix + \"All_residues_abundance.csv\", sep = ',', index=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get the codon counts and the residues counts for each sample and format the dataframe into the correct f\n",
    "\n",
    "codon_abundances = './Data/Analysis_Novaseq/Sel_coff/Codons/'\n",
    "os.makedirs(codon_abundances, exist_ok = True)\n",
    "\n",
    "all_codon_counts=pd.DataFrame()\n",
    "\n",
    "for file in dict_path_to_codons_counts.values():\n",
    "    \n",
    "    ### Import the file\n",
    "    codon_df=pd.read_csv(file, sep=\"\\t\")\n",
    "    \n",
    "    ### Change the first column name\n",
    "    codon_df.rename(columns = {list(codon_df)[0]:'Codon'}, inplace=True)\n",
    "    \n",
    "    ### Unpivot the codon_df to a longer dataframe in wich each row is the read_abundance for each codon at each position\n",
    "    new_codon= pd.melt(codon_df, id_vars=\"Codon\")\n",
    "    new_codon.rename({'variable': 'Position', 'value': 'Count'}, axis=1, inplace=True)\n",
    "\n",
    "      \n",
    "    ### Get the sample ID as it is in metadata\n",
    "    \n",
    "    f1 = file.rstrip(\"_codons.txt\")\n",
    "   \n",
    "    \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/aggregate_dataframes/\")\n",
    "    \n",
    "    sample_name2=sample_name.lstrip(\"Codons\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/Novaseq_R67_bacteria_\")\n",
    "    \n",
    "    ### Add a new column with the sample ID to the codon df\n",
    "    \n",
    "    new_codon['Sample.1'] = sample_name3\n",
    "    \n",
    "    \n",
    "    ### Save the file\n",
    "    new_codon.to_csv(codon_abundances + sample_name3 + '_codons.csv', sep = ',', index=False)\n",
    "\n",
    "    ### I need to concatenate all the dataframes into the same master dataframe \"all_codon_data\"\n",
    "    all_codon_counts=all_codon_counts.append(new_codon,ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "all_residues_counts=pd.DataFrame()\n",
    "\n",
    "for file in dict_path_to_residues_counts.values():\n",
    "    \n",
    "    ### Import the file\n",
    "    residues_df=pd.read_csv(file, sep=\"\\t\")\n",
    "    \n",
    "    ### Change the first column name\n",
    "    residues_df.rename(columns = {list(residues_df)[0]:'Residue'}, inplace=True)\n",
    "    \n",
    "    ### Unpivot the residues_df to a longer dataframe in wich each row is the read_abundance for each codon at each position\n",
    "    new_residue= pd.melt(residues_df, id_vars=\"Residue\")\n",
    "    new_residue.rename({'variable': 'Position', 'value': 'Count'}, axis=1, inplace=True)\n",
    "\n",
    "    ### Get the sample ID as it is in metadata\n",
    "    f1 = file.rstrip(\"_residues.txt\")\n",
    "        \n",
    "    sample_name=f1.lstrip(\"./Data/Analysis_Novaseq/aggregate_dataframes/\")\n",
    "         \n",
    "    sample_name2=sample_name.lstrip(\"Residues\")\n",
    "    \n",
    "    sample_name3=sample_name2.lstrip(\"/Novaseq_R67_bacteria_\")\n",
    "\n",
    "    ### Add a new column with the sample ID to the codon df\n",
    "    \n",
    "    new_residue['Sample.1'] = sample_name3\n",
    "    \n",
    "    \n",
    "#     ### Save the file\n",
    "    new_residue.to_csv(Residues_abundances + sample_name3 + '_residues.csv', sep = ',', index=False)\n",
    "\n",
    "    ### I need to concatenate all the dataframes into the same master dataframe \"all_codon_data\"\n",
    "    all_residues_counts=all_residues_counts.append(new_residue,ignore_index=True)\n",
    "# ### Save the file master file \n",
    "all_residues_counts.to_csv(sel_coff_prefix + \"all_residues_counts.csv\", sep = ',', index=False)    \n",
    "all_codon_counts.to_csv(sel_coff_prefix + \"all_codon_counts.csv\", sep = ',', index=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Merge the counts and the abundance tables into one dataframe \n",
    "\n",
    "all_codons_counts_and_abundances=pd.merge(all_codon_data, all_codon_counts, on=['Codon', 'Position', 'Sample.1'])\n",
    "all_residues_counts_and_abundances=pd.merge(all_residues_data, all_residues_counts, on=['Residue', 'Position', 'Sample.1'])\n",
    "\n",
    "### Save the master file \n",
    "all_residues_counts_and_abundances.to_csv(sel_coff_prefix + \"all_residues_counts_and_abundances.csv\", sep = ',', index=False)    \n",
    "all_codons_counts_and_abundances.to_csv(sel_coff_prefix + \"all_codons_counts_and_abundances.csv\", sep = ',', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Merge the dataframes with the metadata to include the sample information to the read abundance information\n",
    "\n",
    "complete_codons=pd.merge(all_codons_counts_and_abundances, metada, on=\"Sample.1\")\n",
    "complete_residues=pd.merge(all_residues_counts_and_abundances, metada, on=\"Sample.1\")\n",
    "\n",
    "### Add the WT information for each position \n",
    "WT_matrix=pd.read_csv(\"./Data/Analysis_Novaseq/Sel_coff/WT_sequence_table.txt\", sep=\"\\t\")\n",
    "\n",
    "### Convert the position column for all the data frames into a numeric object\n",
    "WT_matrix[\"Position\"] = pd.to_numeric(WT_matrix[\"Position\"])\n",
    "\n",
    "complete_codons[\"Position\"] = pd.to_numeric(complete_codons[\"Position\"])\n",
    "\n",
    "complete_residues[\"Position\"] = pd.to_numeric(complete_residues[\"Position\"])\n",
    "\n",
    "### Merge the files \n",
    "complete_codons_with_WT=pd.merge(complete_codons, WT_matrix, on=\"Position\")\n",
    "complete_residues_with_WT=pd.merge(complete_residues, WT_matrix, on=\"Position\")\n",
    "\n",
    "complete_codons_with_WT['Sample'] = complete_codons_with_WT['ID'].str.slice(0,4)\n",
    "complete_residues_with_WT['Sample'] = complete_residues_with_WT['ID'].str.slice(0,4)\n",
    "\n",
    "### Reorder the columns to \n",
    "\n",
    "codon_cols=[\"ID\", 'Sample', 'Run', 'Date', 'Timepoint', 'Replicate', 'Lane', 'Expected_Reads', 'Raw_Reads', 'ng_PCR_for_library',\n",
    "       'Merged_reads_count_R1_R2', 'Percentage_merged', 'Position','Codon', 'WT_Codon', 'Count', 'read_abundance']\n",
    "complete_codons_with_WT = complete_codons_with_WT.reindex(columns=codon_cols)\n",
    "\n",
    "res_cols=[\"ID\", 'Sample', 'Run', 'Date', 'Timepoint', 'Replicate', 'Lane', 'Expected_Reads', 'Raw_Reads', 'ng_PCR_for_library',\n",
    "       'Merged_reads_count_R1_R2', 'Percentage_merged', 'Position','Residue', 'WT_Residue', 'Count', 'read_abundance']\n",
    "complete_residues_with_WT = complete_residues_with_WT.reindex(columns=res_cols)\n",
    "\n",
    "### Save the dataframes\n",
    "complete_codons_with_WT.to_csv(sel_coff_prefix + \"Complete_codons_DF.csv\", sep = ',', index=False)\n",
    "complete_residues_with_WT.to_csv(sel_coff_prefix + \"Complete_residues_DF.csv\", sep = ',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save the dataframe also in the masterfolder wher I will put together all the runs and continue the analysis\n",
    "\n",
    "complete_codons_with_WT.to_csv(\"../Novaseq_combined/Data/Complete_codons_DF_run2.csv\", sep = ',', index=False)\n",
    "complete_residues_with_WT.to_csv(\"../Novaseq_combined/Data/Complete_residues_DF_run2.csv\", sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUE IN home/Novaseq_combined/ 4_Calculate_sel_coff NOTEBOOK "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
